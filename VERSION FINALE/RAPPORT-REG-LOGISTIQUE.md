# Compte Rendu : R√©gression Logistique pour la Pr√©diction de Remboursement de Pr√™ts

**Auteur** : Analyse bas√©e sur le notebook Kaggle "Simple Logistic Regression"  
**Dataset** : Playground Series S5E11  
**Date** : D√©cembre 2024  
**Score Final** : ROC AUC = 0.9233

---

## üìã Table des Mati√®res

1. [Introduction et Contexte](#1-introduction-et-contexte)
2. [Fondamentaux de la R√©gression Logistique](#2-fondamentaux-de-la-r√©gression-logistique)
3. [Exploration des Donn√©es](#3-exploration-des-donn√©es)
4. [Analyse de Corr√©lation](#4-analyse-de-corr√©lation)
5. [Pr√©traitement des Donn√©es](#5-pr√©traitement-des-donn√©es)
6. [Construction du Mod√®le](#6-construction-du-mod√®le)
7. [√âvaluation et Validation](#7-√©valuation-et-validation)
8. [R√©sultats et Interpr√©tation](#8-r√©sultats-et-interpr√©tation)
9. [Annexes Techniques](#9-annexes-techniques)

---

## 1. Introduction et Contexte

### 1.1 Objectif du Projet

Ce projet vise √† construire un mod√®le de **r√©gression logistique** pour pr√©dire si un emprunteur remboursera son pr√™t. Il s'agit d'un probl√®me de **classification binaire** fondamental dans le domaine du cr√©dit et de la gestion des risques financiers.

### 1.2 Pourquoi la R√©gression Logistique ?

La r√©gression logistique est choisie pour plusieurs raisons :

- ‚úÖ **Simplicit√©** : Mod√®le lin√©aire facile √† comprendre et √† impl√©menter
- ‚úÖ **Interpr√©tabilit√©** : Les coefficients ont une signification claire
- ‚úÖ **Efficacit√©** : Rapide √† entra√Æner, m√™me sur de grands datasets
- ‚úÖ **Probabilit√©s** : Fournit des scores de probabilit√©, pas seulement des classes
- ‚úÖ **Baseline** : Excellent point de d√©part avant des mod√®les plus complexes

### 1.3 Importance Pratique

Dans le secteur financier, pr√©dire le remboursement des pr√™ts permet de :

1. **R√©duire les risques** : Identifier les emprunteurs √† risque
2. **Optimiser les d√©cisions** : Approuver les bons pr√™ts, refuser les mauvais
3. **G√©rer le capital** : Allouer efficacement les ressources
4. **Respecter les r√©gulations** : Justifier les d√©cisions de cr√©dit

---

## 2. Fondamentaux de la R√©gression Logistique

### 2.1 Principe G√©n√©ral

La r√©gression logistique transforme une combinaison lin√©aire de features en une **probabilit√©** entre 0 et 1.

**Processus complet** :

```
√âtape 1: Combinaison Lin√©aire
z = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô + b

√âtape 2: Fonction Sigmo√Øde
P(y=1|x) = œÉ(z) = 1 / (1 + e‚Åª·∂ª)

√âtape 3: D√©cision
Classe pr√©dite = {1 si P ‚â• 0.5
                 {0 si P < 0.5
```

### 2.2 La Fonction Sigmo√Øde

**√âquation** :
```
œÉ(z) = 1 / (1 + e^(-z))
```

**Propri√©t√©s math√©matiques** :
- Domaine : z ‚àà ‚Ñù (tous les r√©els)
- Image : œÉ(z) ‚àà [0, 1] (probabilit√©)
- Point d'inflexion : œÉ(0) = 0.5
- Limites : lim(z‚Üí+‚àû) œÉ(z) = 1, lim(z‚Üí-‚àû) œÉ(z) = 0

**Visualisation de la courbe sigmo√Øde** :

```
P(y=1)
  1.0 ‚îÇ           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
      ‚îÇ         ‚ï±
      ‚îÇ       ‚ï±
  0.5 ‚îÇ     ‚ï±    ‚Üê Seuil de d√©cision
      ‚îÇ   ‚ï±
      ‚îÇ ‚ï±
  0.0 ‚îÇ‚îò
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí z
       -6  -4  -2   0   2   4   6
```

### 2.3 Fonction de Co√ªt (Log Loss)

Pour entra√Æner le mod√®le, on minimise la **log loss** (entropie crois√©e binaire) :

```
J(w, b) = -1/m Œ£·µ¢‚Çå‚ÇÅ·µê [y·µ¢¬∑log(≈∑·µ¢) + (1-y·µ¢)¬∑log(1-≈∑·µ¢)]

o√π :
  - m = nombre d'exemples
  - y·µ¢ = vraie classe (0 ou 1)
  - ≈∑·µ¢ = probabilit√© pr√©dite P(y=1|x·µ¢)
```

**Intuition** :
- Si y=1 et ≈∑‚Üí1 : log(≈∑)‚Üí0, co√ªt faible ‚úì
- Si y=1 et ≈∑‚Üí0 : log(≈∑)‚Üí-‚àû, co√ªt √©lev√© ‚úó
- Le mod√®le est p√©nalis√© pour les pr√©dictions confiantes mais fausses

### 2.4 Optimisation : Gradient Descent

**Algorithme** :

```
Initialiser w et b al√©atoirement
Pour chaque it√©ration jusqu'√† convergence :
    1. Calculer les pr√©dictions : ≈∑ = œÉ(Xw + b)
    2. Calculer le gradient : ‚àÇJ/‚àÇw = (1/m)X·µÄ(≈∑ - y)
    3. Mettre √† jour : w := w - Œ±¬∑(‚àÇJ/‚àÇw)
                       b := b - Œ±¬∑(‚àÇJ/‚àÇb)
o√π Œ± = learning rate
```

### 2.5 R√©gularisation

Pour √©viter le **sur-apprentissage**, on ajoute un terme de p√©nalit√© :

**R√©gularisation L2 (Ridge)** :
```
J_reg(w) = J(w) + Œª¬∑||w||¬≤

o√π :
  - Œª = param√®tre de r√©gularisation
  - ||w||¬≤ = somme des carr√©s des coefficients
```

**En scikit-learn** : Le param√®tre `C` est l'inverse de Œª
```python
C = 1/Œª
# C petit ‚Üí forte r√©gularisation
# C grand ‚Üí faible r√©gularisation
```

---

## 3. Exploration des Donn√©es

### 3.1 Structure du Dataset

**Caract√©ristiques g√©n√©rales** :

| Dataset | Nombre de lignes | Fichier |
|---------|------------------|---------|
| Entra√Ænement | 10,000 | `train.csv` |
| Test | 5,000 | `test.csv` |
| **Total** | **15,000** | - |

### 3.2 Variables du Dataset

#### Variables Num√©riques

| Variable | Type | Plage | Description |
|----------|------|-------|-------------|
| `id` | Identifiant | 0 - 9,999 | Identifiant unique |
| `credit_score` | Discret | 300 - 849 | Score de cr√©dit |
| `debt_to_income_ratio` | Continu | 0.10 - 0.60 | Ratio dette/revenu |
| `interest_rate` | Continu | 3% - 15% | Taux d'int√©r√™t du pr√™t |

#### Variables Cat√©gorielles

| Variable | Modalit√©s | Description |
|----------|-----------|-------------|
| `gender` | Male, Female | Genre de l'emprunteur |
| `marital_status` | Single, Married, Divorced | Statut matrimonial |
| `education_level` | High School, Bachelors, Masters, PhD | Niveau d'√©ducation |
| `employment_status` | Employed, Unemployed, Self-Employed | Statut d'emploi |
| `loan_purpose` | Home, Car, Education, Other | But du pr√™t |
| `grade_subgrade` | A1, B2, C3, D4 | Grade du pr√™t |

#### Variable Cible

| Variable | Type | Valeurs | Description |
|----------|------|---------|-------------|
| **`loan_paid_back`** | **Binaire** | **0, 1** | **0 = Non rembours√©, 1 = Rembours√©** |

### 3.3 Statistiques Descriptives

#### Variables Num√©riques

```
credit_score:
  ‚îú‚îÄ count: 10,000
  ‚îú‚îÄ mean:  574.88
  ‚îú‚îÄ std:   159.00
  ‚îú‚îÄ min:   300
  ‚îú‚îÄ 25%:   436
  ‚îú‚îÄ 50%:   573
  ‚îú‚îÄ 75%:   714
  ‚îî‚îÄ max:   849

debt_to_income_ratio:
  ‚îú‚îÄ count: 10,000
  ‚îú‚îÄ mean:  0.3499
  ‚îú‚îÄ std:   0.1436
  ‚îú‚îÄ min:   0.1000
  ‚îú‚îÄ 25%:   0.2260
  ‚îú‚îÄ 50%:   0.3510
  ‚îú‚îÄ 75%:   0.4730
  ‚îî‚îÄ max:   0.5999

interest_rate:
  ‚îú‚îÄ count: 10,000
  ‚îú‚îÄ mean:  9.01%
  ‚îú‚îÄ std:   3.45%
  ‚îú‚îÄ min:   3.00%
  ‚îú‚îÄ 25%:   6.09%
  ‚îú‚îÄ 50%:   9.01%
  ‚îú‚îÄ 75%:   12.01%
  ‚îî‚îÄ max:   15.00%

loan_paid_back (CIBLE):
  ‚îú‚îÄ count: 10,000
  ‚îú‚îÄ mean:  0.4968
  ‚îú‚îÄ std:   0.5000
  ‚îú‚îÄ min:   0
  ‚îú‚îÄ 25%:   0
  ‚îú‚îÄ 50%:   0
  ‚îú‚îÄ 75%:   1
  ‚îî‚îÄ max:   1
```

### 3.4 Distribution de la Variable Cible

**R√©partition des classes** :

```
Classe 0 (Non rembours√©) : 5,032 exemples (50.32%)
Classe 1 (Rembours√©)     : 4,968 exemples (49.68%)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total                    : 10,000

Visualisation:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Not Paid Back ‚îÇ Paid Back          ‚îÇ
‚îÇ     5032      ‚îÇ    4968            ‚îÇ
‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       ‚îÇ
‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       ‚îÇ
‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      50.32%          49.68%
```

**Observation critique** :
- ‚úÖ **Classes parfaitement √©quilibr√©es** (~50/50)
- ‚úÖ Pas besoin de techniques de r√©√©chantillonnage (SMOTE, undersampling)
- ‚úÖ M√©trique accuracy pertinente (pas de classe majoritaire)

### 3.5 Cardinalit√© des Variables

```
Nombre de valeurs uniques par variable:

id                      : 10,000  (tous uniques)
credit_score            : 550     (discret)
gender                  : 2       (binaire)
marital_status          : 3       (cat√©goriel)
debt_to_income_ratio    : 10,000  (continu)
education_level         : 4       (ordinal)
employment_status       : 3       (cat√©goriel)
loan_purpose            : 4       (cat√©goriel)
grade_subgrade          : 4       (ordinal)
interest_rate           : 10,000  (continu)
loan_paid_back          : 2       (binaire - CIBLE)
```

---

## 4. Analyse de Corr√©lation

### 4.1 Matrice de Corr√©lation des Variables Num√©riques

**Variables analys√©es** :
- `credit_score`
- `debt_to_income_ratio`
- `interest_rate`
- `loan_paid_back` (cible)

**R√©sultats (valeurs approximatives)** :

```
Corr√©lation avec loan_paid_back:
‚îú‚îÄ credit_score          : +0.05  (corr√©lation tr√®s faible positive)
‚îú‚îÄ debt_to_income_ratio  : -0.03  (corr√©lation tr√®s faible n√©gative)
‚îî‚îÄ interest_rate         : -0.02  (corr√©lation tr√®s faible n√©gative)

Intercorr√©lations:
‚îú‚îÄ credit_score ‚Üî interest_rate        : -0.10
‚îú‚îÄ credit_score ‚Üî debt_to_income_ratio : +0.02
‚îî‚îÄ interest_rate ‚Üî debt_to_income_ratio: +0.03
```

### 4.2 Interpr√©tation

**Constatations principales** :

1. **Faibles corr√©lations lin√©aires** : Aucune variable num√©rique n'a une corr√©lation forte avec la cible
   - Cela sugg√®re que les relations sont **non-lin√©aires** ou que les **variables cat√©gorielles** sont plus importantes

2. **Absence de multicolin√©arit√©** : Les variables explicatives ne sont pas fortement corr√©l√©es entre elles
   - ‚úÖ Bon pour la stabilit√© du mod√®le
   - ‚úÖ Pas de redondance d'information

3. **Importance des variables cat√©gorielles** : Les features comme `grade_subgrade`, `employment_status`, etc. pourraient avoir plus de pouvoir pr√©dictif

### 4.3 Visualisation

```
Heatmap de corr√©lation (repr√©sentation textuelle):

                      credit  debt_to  interest  loan_paid
                      score   income   rate      back
credit_score          1.00    0.02    -0.10     0.05
debt_to_income_ratio  0.02    1.00     0.03    -0.03
interest_rate        -0.10    0.03     1.00    -0.02
loan_paid_back        0.05   -0.03    -0.02     1.00

L√©gende:
  1.00 : Corr√©lation parfaite
  0.70+: Corr√©lation forte
  0.30+: Corr√©lation mod√©r√©e
  0.10+: Corr√©lation faible
  0.00 : Aucune corr√©lation
```

---

## 5. Pr√©traitement des Donn√©es

### 5.1 Vue d'Ensemble du Pipeline

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Donn√©es Brutes ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. S√©paration Cible/ID  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. Identification Types ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. One-Hot Encoding     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 4. Standardisation      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Donn√©es Pr√™tes pour ML ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 5.2 √âtape 1 : S√©paration Cible et Identifiants

**Code** :
```python
# Extraction de la variable cible
y = X_full.pop('loan_paid_back')

# Conservation des IDs test pour la soumission finale
testID = X_test.pop('id')

# Suppression de l'ID du dataset d'entra√Ænement
X_full.drop('id', axis=1, inplace=True)
```

**R√©sultat** :
- `y` : vecteur de 10,000 valeurs (0 ou 1)
- `X_full` : DataFrame de 10,000 √ó 9 features
- `testID` : vecteur de 5,000 IDs pour reconstituer la soumission

### 5.3 √âtape 2 : Identification des Types de Variables

**D√©cision de traitement** :

```python
# Variables √† encoder (cat√©gorielles + pseudo-num√©riques)
cat_cols = [
    'credit_score',           # Discret ‚Üí Cat√©goriel
    'gender',                 # Cat√©goriel
    'marital_status',         # Cat√©goriel
    'debt_to_income_ratio',   # Continu ‚Üí Cat√©goriel (choix de l'auteur)
    'education_level',        # Cat√©goriel
    'employment_status',      # Cat√©goriel
    'loan_purpose',           # Cat√©goriel
    'grade_subgrade'          # Cat√©goriel
]

# Variables num√©riques continues
num_cols = ['interest_rate']  # Seule variable vraiment continue
```

**Justification** :
- `credit_score` : Bien que num√©rique, a seulement 550 valeurs distinctes ‚Üí trait√© comme cat√©goriel
- `debt_to_income_ratio` : Choix de l'auteur de le traiter comme cat√©goriel
- Cette approche peut capturer des **relations non-lin√©aires** plus complexes

### 5.4 √âtape 3 : One-Hot Encoding

#### Principe

Le **One-Hot Encoding** transforme chaque variable cat√©gorielle en plusieurs colonnes binaires (0/1).

**Exemple** :

```
Avant encoding:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ gender ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Male   ‚îÇ
‚îÇ Female ‚îÇ
‚îÇ Male   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Apr√®s encoding:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ gender_Male‚îÇ gender_Female‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ     1      ‚îÇ      0       ‚îÇ
‚îÇ     0      ‚îÇ      1       ‚îÇ
‚îÇ     1      ‚îÇ      0       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### Impl√©mentation

```python
from sklearn.preprocessing import OneHotEncoder

# Initialisation
Oh = OneHotEncoder(
    handle_unknown='ignore',  # Ignore les cat√©gories inconnues dans le test
    sparse_output=False       # Retourne un array dense (pas sparse)
)

# Encoding des donn√©es d'entra√Ænement
X_encoded = pd.DataFrame(Oh.fit_transform(X_full[cat_cols]))

# Encoding des donn√©es de test (avec les m√™mes cat√©gories)
test_encoded = pd.DataFrame(Oh.transform(X_test[cat_cols]))
```

#### R√©sultat

**Dimensions** :

```
Avant encoding:
  X_full: 10,000 √ó 8 colonnes (cat_cols)

Apr√®s encoding:
  X_encoded: 10,000 √ó N colonnes
  o√π N = nombre total de modalit√©s de toutes les variables cat√©gorielles

Exemple de calcul:
  gender (2) + marital_status (3) + education_level (4) + 
  employment_status (3) + loan_purpose (4) + grade_subgrade (4) +
  credit_score (550) + debt_to_income_ratio (10,000)
  = Beaucoup de colonnes !
```

**Note** : Le nombre exact de colonnes d√©pend des modalit√©s pr√©sentes dans le dataset.

### 5.5 √âtape 4 : Jonction avec Variables Num√©riques

```python
# Ajout de la variable num√©rique continue
X = X_encoded.join(X_full[num_cols])
test = test_encoded.join(X_test[num_cols])

# Conversion des noms de colonnes en string (pour √©viter les erreurs)
X.columns = X.columns.astype(str)
test.columns = test.columns.astype(str)
```

### 5.6 √âtape 5 : Standardisation (Scaling)

#### Importance Critique

**Pourquoi standardiser ?**

1. **√âchelles diff√©rentes** : Les variables ont des plages tr√®s diff√©rentes
   - `credit_score` : [300, 849]
   - `interest_rate` : [3, 15]
   - Colonnes one-hot : [0, 1]

2. **Gradient descent** : La convergence est plus rapide avec des features standardis√©es

3. **R√©gularisation** : La p√©nalit√© L2 doit traiter toutes les features √©quitablement

#### Formule de Standardisation (Z-score)

```
Pour chaque feature j:

x'_ij = (x_ij - Œº_j) / œÉ_j

o√π :
  x_ij  : valeur originale (exemple i, feature j)
  Œº_j   : moyenne de la feature j
  œÉ_j   : √©cart-type de la feature j
  x'_ij : valeur standardis√©e

R√©sultat: x' ~ N(0, 1) (distribution approximativement normale)
```

#### Impl√©mentation

```python
from sklearn.preprocessing import StandardScaler

# Initialisation du scaler
scaler = StandardScaler()

# Fit sur train, transform train et test
X_scaled = pd.DataFrame(scaler.fit_transform(X))
test_scaled = pd.DataFrame(scaler.transform(test))
```

**‚ö†Ô∏è Important** :
- Le scaler est **fit sur train uniquement** (pas sur test)
- Puis appliqu√© √† train ET test avec les m√™mes param√®tres (Œº et œÉ du train)
- Cela √©vite le **data leakage**

#### Exemple de Transformation

```
Variable: interest_rate

Avant standardisation:
  min = 3.00, max = 15.00, mean = 9.01, std = 3.45

Apr√®s standardisation:
  Pour x = 3.00  : z = (3.00 - 9.01) / 3.45 = -1.74
  Pour x = 9.01  : z = (9.01 - 9.01) / 3.45 =  0.00
  Pour x = 15.00 : z = (15.00 - 9.01) / 3.45 = +1.74

Nouvelle distribution:
  min ‚âà -1.74, max ‚âà +1.74, mean = 0, std = 1
```

---

## 6. Construction du Mod√®le

### 6.1 Split Train/Validation

**Objectif** : S√©parer les donn√©es pour √©valuer le mod√®le sur un ensemble qu'il n'a jamais vu.

```python
from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(
    X, y,
    test_size=0.2,      # 20% pour validation
    random_state=42     # Pour reproductibilit√©
)
```

**R√©partition** :

```
Dataset Total: 10,000 exemples
‚îú‚îÄ Train      : 8,000 exemples (80%)
‚îÇ              ‚îî‚îÄ Utilis√©s pour apprendre les coefficients w
‚îÇ
‚îî‚îÄ Validation : 2,000 exemples (20%)
               ‚îî‚îÄ Utilis√©s pour √©valuer la performance
```

### 6.2 Configuration du Mod√®le

```python
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(
    C=1e-3,         # Param√®tre de r√©gularisation (Œª = 1/C = 1000)
    max_iter=1000,  # Nombre max d'it√©rations du gradient descent
    solver='lbfgs', # Algorithme d'optimisation (par d√©faut)
    random_state=42 # Pour reproductibilit√©
)
```

#### Param√®tre C (R√©gularisation)

**Relation** : C = 1/Œª

```
C = 1e-3 = 0.001
  ‚Üì
Œª = 1/C = 1000

Effet:
‚îú‚îÄ Œª √©lev√© (C faible) ‚Üí FORTE r√©gularisation
‚îÇ                      ‚Üí Coefficients w proches de 0
‚îÇ                      ‚Üí Mod√®le simple, moins de sur-apprentissage
‚îÇ
‚îî‚îÄ Œª faible (C grand) ‚Üí FAIBLE r√©gularisation
                       ‚Üí Coefficients w peuvent √™tre grands
                       ‚Üí Mod√®le complexe, risque de sur-apprentissage
```

**Choix C=1e-3** :
- ‚úÖ R√©gularisation tr√®s forte
- ‚úÖ Adapt√© pour √©viter le sur-apprentissage avec beaucoup de features (one-hot encoding)
- ‚úÖ Privil√©gie la g√©n√©ralisation sur l'ajustement parfait aux donn√©es d'entra√Ænement

### 6.3 Entra√Ænement du Mod√®le

```python
# Entra√Ænement
model.fit(X_train, y_train)

# Le mod√®le apprend les coefficients w et le biais b
# en minimisant la log loss r√©gularis√©e :
# J(w) = -1/m Œ£[y¬∑log(≈∑) + (1-y)¬∑log(1-≈∑)] + Œª¬∑||w||¬≤
```

**Processus interne** :

```
It√©ration 1:
  1. Initialiser w et b al√©atoirement
  2. Calculer pr√©dictions: ≈∑ = œÉ(Xw + b)
  3. Calculer co√ªt: J(w)
  4. Calculer gradients: ‚àÇJ/‚àÇw
  5. Mettre √† jour: w := w - Œ±¬∑(‚àÇJ/‚àÇw)

It√©ration 2:
  [r√©p√©ter √©tapes 2-5]
  ...

It√©ration N:
  [convergence atteinte ou max_iter=1000]
```

---

## 7. √âvaluation et Validation

### 7.1 M√©triques pour la Classification Binaire

#### ROC AUC (M√©trique Principale)

**D√©finition** : Area Under the Receiver Operating Characteristic Curve

**Interpr√©tation** :

```
AUC = 1.0   ‚Üí Mod√®le parfait (s√©pare parfaitement les classes)
AUC = 0.9+  ‚Üí Excellent mod√®le
AUC = 0.8+  ‚Üí Tr√®s bon mod√®le
AUC = 0.7+  ‚Üí Bon mod√®le
AUC = 0.6+  ‚Üí Mod√®le moyen
AUC = 0.5   ‚Üí Mod√®le al√©atoire (inutile)
AUC < 0.5   ‚Üí Mod√®le pire que al√©atoire
```

**Avantages** :
- ‚úÖ Insensible au d√©s√©quilibre des classes (mais ici √©quilibr√©es)
- ‚úÖ Mesure la capacit√© de discrimination globale
- ‚úÖ Ind√©pendant du seuil de d√©cision choisi

#### Courbe ROC

**Construction** :

```
Pour chaque seuil t de 0 √† 1:
  1. Classer y_pred ‚â• t comme classe 1
  2. Calculer TPR (True Positive Rate) = TP / (TP + FN)
  3. Calculer FPR (False Positive Rate) = FP / (FP + TN)
  4. Tracer le point (FPR, TPR)

AUC = aire sous la courbe trac√©e
```

**Visualisation** :

```
TPR ‚îÇ
1.0 ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  AUC = 0.92
    ‚îÇ ‚îÇ         ‚îÇ
0.8 ‚îÇ ‚îÇ         ‚îÇ
    ‚îÇ ‚îÇ         ‚îÇ
0.6 ‚îÇ ‚îÇ         ‚îÇ
    ‚îÇ‚ï±          ‚îÇ
0.4 ‚îÇ           ‚îÇ
    ‚îÇ           ‚îÇ
0.2 ‚îÇ           ‚îÇ
    ‚îÇ           ‚îÇ
0.0 ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    0   0.2  0.4  0.6  0.8  1.0
                FPR

L√©gende:
  Ligne bleue: Notre mod√®le (AUC=0.92)
  Diagonale: Mod√®le al√©atoire (AUC=0.5)
  Coin sup√©rieur gauche: Mod√®le parfait (AUC=1.0)
```

### 7.2 Validation Hold-Out

**M√©thode** : √âvaluer sur l'ensemble de validation s√©par√©

```python
# Pr√©dictions sur validation
y_pred_proba = model.predict_proba(X_val)[:, 1]  # Probabilit√©s classe 1

# Calcul du score
from sklearn.metrics import roc_auc_score
score_val = roc_auc_score(y_val, y_pred_proba)

print(f"ROC AUC (Validation): {score_val:.4f}")
# R√©sultat attendu: ~0.92
```

**Limites** :
- ‚ùå Un seul split : le score peut varier selon le split
- ‚ùå Perte de donn√©es (20% non utilis√©s pour l'entra√Ænement)

### 7.3 Validation Crois√©e (Cross-Validation)

**M√©thode** : K-Fold Cross-Validation (K=5)

```
Principe:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Dataset complet (10,000)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üì Division en 5 folds
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Fold 1 ‚îÇ Fold 2 ‚îÇ Fold 3 ‚îÇ Fold 4 ‚îÇ Fold 5 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  2,000    2,000    2,000    2,000    2,000

It√©ration 1: Train [2,3,4,5], Test [1]
It√©ration 2: Train [1,3,4,5], Test [2]
It√©ration 3: Train [1,2,4,5], Test [